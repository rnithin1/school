@article{Abdel-Hamid2014,
  title={{Convolutional neural networks for speech recognition}},
  author={Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  journal={IEEE TASLP},
  volume={22},
  number={10},
  pages={1533--1545},
  year={2014},
  publisher={IEEE}
}

@inproceedings{Anden2015,
abstract = {We introduce the joint time-frequency scattering transform, a time shift invariant descriptor of time-frequency structure for audio classification. It is obtained by applying a two-dimensional wavelet transform in time and log-frequency to a time-frequency wavelet scalogram. We show that this descriptor successfully characterizes complex time-frequency phenomena such as time-varying filters and frequency modulated excitations. State-of-the-art results are achieved for signal reconstruction and phone segment classification on the TIMIT dataset.},
archivePrefix = {arXiv},
arxivId = {1512.02125},
author = {And{\'{e}}n, Joakim and Lostanlen, Vincent and Mallat, St{\'{e}}phane},
booktitle = {Proc. MLSP},
doi = {10.1109/MLSP.2015.7324385},
eprint = {1512.02125},
isbn = {9781467374545},
title = {{Joint time-frequency scattering for audio classification}},
url = {http://arxiv.org/abs/1512.02125
http://dx.doi.org/10.1109/MLSP.2015.7324385},
year = {2015}
}

@inproceedings{Bittner2014,
abstract = {We introduce MedleyDB: a dataset of annotated, royalty-free multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research.},
author = {Bittner, Rachel and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan},
booktitle = {Proc. ISMIR},
title = {{MedleyDB: a multitrack dataset for annotation-intensive MIR research}},
url = {http://marl.smusic.nyu.edu/medleydb{\_}webfiles/bittner{\_}medleydb{\_}ismir2014.pdf},
year = {2014}
}

@inproceedings{Choi2015,
author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark and Kim, Jeonghee and Labs, Naver},
title = {{Auralisation of deep convolutional neural networks: listening to learned features}},
url = {http://ismir2015.uma.es/LBD/LBD24.pdf},
year = {2015},
booktitle = {Proc. ISMIR}
}

@misc{Chollet2015,
author = {Fran\c{c}ois Chollet},
title = {{Keras: a deep learning library for Theano and TensorFlow}},
year = {2015},
publisher = {GitHub},
journal = {GitHub repository},
}

@incollection{deCheveigne2005,
author = {de Cheveign{\'{e}}, Alain},
booktitle = {Oxford Handbook of Auditory Science: Hearing},
chapter = {4},
doi = {10.1093/oxfordhb/9780199233557.013.0004},
isbn = {9780199233557},
pages = {71--104},
title = {{Pitch perception}},
url = {http://audition.ens.fr/adc/pdf/2010{\_}Pitch{\_}OUPHAS.pdf},
year = {2005},
publisher = {Oxford University Press}
}

@inproceedings{Dieleman2011,
abstract = {Recently the ?Million Song Dataset?, containing audio features and metadata for one million songs, was made available. In this paper, we build a convolutional network that is then trained to perform artist recognition, genre recognition and key detection. The network is tailored to summarize the audio features over musically significant timescales. It is infeasible to train the network on all available data in a supervised fashion, so we use unsupervised pretraining to be able to harness the entire dataset: we train a convolutional deep belief network on all data, and then use the learnt parameters to initialize a convolutional multilayer perceptron with the same architecture. The MLP is then trained on a labeled subset of the data for each task. We also train the same MLP with randomly initialized weights. We find that our convolutional approach improves accuracy for the genre recognition and artist recognition tasks. Unsupervised pretraining improves convergence speed in all cases. For artist recognition it improves accuracy as well.},
author = {Dieleman, Sander and Brakel, Phil\'{e}mon and Schrauwen, Benjamin},
isbn = {9780615548654},
booktitle = {Proc. ISMIR},
title = {{Audio-based music classification with a pretrained convolutional network}},
url = {https://biblio.ugent.be/publication/1989534},
year = {2011}
}

@inproceedings{Dieleman2014,
abstract = {Content-based music information retrieval tasks have traditionally been solved using engineered features and shallow processing architectures. In recent years, there has been increasing interest in using feature learning and deep architectures instead, thus reducing the required engineering effort and the need for prior knowledge. However, this new approach typically still relies on mid-level representations of music audio, e.g. spectrograms, instead of raw audio signals. In this paper, we investigate whether it is possible to apply feature learning directly to raw audio signals. We train convolutional neural networks using both approaches and compare their performance on an automatic tagging task. Although they do not outperform a spectrogram-based approach, the networks are able to autonomously discover frequency decompositions from raw audio, as well as phase-and translation-invariant feature representations. ï¿½ 2014 IEEE.},
author = {Dieleman, Sander and Schrauwen, Benjamin},
doi = {10.1109/ICASSP.2014.6854950},
isbn = {9781479928927},
issn = {15206149},
booktitle = {Proc. ICASSP},
keywords = {automatic tagging,convolutional neural networks,end-to-end learning,feature learning,music information retrieval},
title = {{End-to-end learning for music audio}},
url = {http://www.redes.unb.br/lasp/files/events/ICASSP2014/papers/p7014-dieleman.pdf},
year = {2014}
}

@inproceedings{Durand2016,
title = {Feature-adapted convolutional neural networks for downbeat tracking},
author = {Durand, Simon and Bello, Juan P. and David, Bertrand and Richard, Ga\"{e}l},
booktitle = {Proc. ICASSP},
year = {2016}}

@inproceedings{Eronen2000,
  title={Musical instrument recognition using cepstral coefficients and temporal features},
  author={Eronen, Antti and Klapuri, Anssi},
  booktitle={Proc. ICASSP},
  year={2000},
}

@inproceedings{Goto2003,
  title={{RWC music database: music genre database and musical instrument sound database.}},
  author={Goto, Masataka and Hashiguchi, Hiroki and Nishimura, Takuichi and Oka, Ryuichi},
  booktitle = {Proc. ISMIR},
  year={2003}
}

@inproceedings{Hamel2012,
abstract = {Low-level aspects of music audio such as timbre, loud- ness and pitch, can be relatively well modelled by features extracted from short-time windows. Higher-level aspects such as melody, harmony, phrasing and rhythm, on the other hand, are salient only at larger timescales and re- quire a better representation of time dynamics. For var- ious music information retrieval tasks, one would bene?t from modelling both low and high level aspects in a uni- ?ed feature extraction framework. By combining adaptive features computed at different timescales, short-timescale events are put in context by detecting longer timescale fea- tures. In this paper, we describe a method to obtain such multi-scale features and evaluate its effectiveness for auto- matic tag annotation.},
author = {Hamel, Philippe and Bengio, Yoshua and Eck, Douglas},
booktitle = {Proc. ISMIR},
isbn = {9789727521449},
title = {{Building musically-relevant audio features through multiple timescale representations}},
url = {http://ismir2012.ismir.net/event/papers/553{\_}ISMIR{\_}2012.pdf},
year = {2012}
}

@inproceedings{He2015,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
booktitle = {Proc. ICCV},
year = {2015}
}

@inproceedings{Humphrey2012tonnetz,
  title={Learning a robust tonnetz-space transform for automatic chord recognition},
  author={Humphrey, Eric J. and Cho, Taemin and Bello, Juan P.},
  booktitle={Proc. ICASSP},
  year={2012},
}

@article{Humphrey2013,
abstract = {As we look to advance the state of the art in content-based music informatics, there is a general sense that progress is decelerating throughout the field. On closer inspection, performance trajectories across several applications reveal that this is indeed the case, raising some difficult questions for the discipline: why are we slowing down, and what can we do about it? Here, we strive to address both of these concerns. First, we critically review the standard approach to music signal analysis and identify three specific deficiencies to current methods: hand-crafted feature design is sub-optimal and unsustainable, the power of shallow architectures is fundamentally limited, and short-time analysis cannot encode musically meaningful structure. Acknowledging breakthroughs in other perceptual AI domains, we offer that deep learning holds the potential to overcome each of these obstacles. Through conceptual arguments for feature learning and deeper processing architectures, we demonstrate how deep processing models are more powerful extensions of current methods, and why now is the time for this paradigm shift. Finally, we conclude with a discussion of current challenges and the potential impact to further motivate an exploration of this promising research area.},
author = {Humphrey, Eric J. and Bello, Juan P. and Le Cun, Yann},
doi = {10.1007/s10844-013-0248-5},
issn = {09259902},
journal = {JIIS},
keywords = {Deep learning,Music informatics,Signal processing},
number = {3},
pages = {461--481},
title = {{Feature learning and deep architectures: New directions for music informatics}},
url = {http://yann.lecun.com/exdb/publis/pdf/humphrey-jiis-13.pdf},
volume = {41},
year = {2013}
}


@article{Joder2009,
abstract = {Nowadays, it appears essential to design automatic indexing tools which provide meaningful and efficient means to describe the musical audio content. There is in fact a growing interest for music information retrieval (MIR) applications amongst which the most popular are related to music similarity retrieval, artist identification, musical genre or instrument recognition. Current MIR-related classification systems usually do not take into account the mid-term temporal properties of the signal (over several frames) and lie on the assumption that the observations of the features in different frames are statistically independent. The aim of this paper is to demonstrate the usefulness of the information carried by the evolution of these characteristics over time. To that purpose, we propose a number of methods for early and late temporal integration and provide an in-depth experimental study on their interest for the task of musical instrument recognition on solo musical phrases. In particular, the impact of the time horizon over which the temporal integration is performed will be assessed both for fixed and variable frame length analysis. Also, a number of proposed alignment kernels will be used for late temporal integration. For all experiments, the results are compared to a state of the art musical instrument recognition system.},
author = {Joder, Cyril and Essid, Slim and Richard, Ga{\"{e}}l},
doi = {10.1109/TASL.2008.2007613},
issn = {15587916},
journal = {IEEE TASLP},
keywords = {Alignment kernels,Audio classification,Music information retrieval (MIR),Musical instrument recognition,Support vector machine (SVM),Temporal feature integration},
number = {1},
pages = {174--186},
title = {Temporal integration for audio classification with application to musical instrument classification},
url = {http://biblio.telecom-paristech.fr/cgi-bin/download.cgi?id=8585},
volume = {17},
year = {2009}
}

@article{Kereliuk2015,
abstract = {An adversary is essentially an algorithm intent on making a classification system perform in some particular way given an input, e.g., increase the probability of a false negative. Recent work builds adversaries for deep learning systems applied to image object recognition, which exploits the parameters of the system to find the minimal perturbation of the input image such that the network misclassifies it with high confidence. We adapt this approach to construct and deploy an adversary of deep learning systems applied to music content analysis. In our case, however, the input to the systems is magnitude spectral frames, which requires special care in order to produce valid input audio signals from network-derived perturbations. For two different train-test partitionings of two benchmark datasets, and two different deep architectures, we find that this adversary is very effective in defeating the resulting systems. We find the convolutional networks are more robust, however, compared with systems based on a majority vote over individually classified audio frames. Furthermore, we integrate the adversary into the training of new deep systems, but do not find that this improves their resilience against the same adversary.},
archivePrefix = {arXiv},
arxivId = {1507.04761},
author = {Kereliuk, Corey and Sturm, Bob L. and Larsen, Jan},
doi = {10.1109/TMM.2015.2478068},
eprint = {1507.04761},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/01005e6cff03ae253c088741b53ffe4a7280450e.pdf:pdf},
issn = {15209210},
journal = {IEEE Trans. Multimedia},
keywords = {AEA-MIR content-based processing and music information retrieval,deep learning},
number = {11},
pages = {2059--2071},
title = {{Deep Learning and Music Adversaries}},
url = {http://www2.imm.dtu.dk/pubdb/views/edoc{\_}download.php/6904/pdf/imm6904.pdf},
volume = {17},
year = {2015}
}

@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v5},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v5},
booktitle = {Proc. ICML},
title = {Adam: a Method for Stochastic Optimization},
url = {http://arxiv.org/pdf/1412.6980v8.pdf},
year = {2015}
}

@article{Li2015,
abstract = {Traditional methods to tackle many music information retrieval tasks typically follow a two-step architecture: feature engineering followed by a simple learning algorithm. In these "shallow" architectures, feature engineering and learning are typically disjoint and unrelated. Additionally, feature engineering is difficult, and typically depends on extensive domain expertise. In this paper, we present an application of convolutional neural networks for the task of automatic musical instrument identification. In this model, feature extraction and learning algorithms are trained together in an end-to-end fashion. We show that a convolutional neural network trained on raw audio can achieve performance surpassing traditional methods that rely on hand-crafted features.},
archivePrefix = {arXiv},
arxivId = {1511.05520},
author = {Li, Peter and Qian, Jiyuan and Wang, Tian},
eprint = {1511.05520},
journal = {arXiv preprint},
volume = {1511.05520},
title = {Automatic Instrument Recognition in Polyphonic Music Using Convolutional Neural Networks},
url = {http://arxiv.org/abs/1511.05520},
year = {2015}
}

@misc{McFee2015-librosa,
  author       = {Brian McFee and
                  Matt McVicar and
                  Colin Raffel and
                  Dawen Liang and
                  Oriol Nieto and
                  Eric Battenberg and
                  Josh Moore and
                  Dan Ellis and
                  Ryuichi Yamamoto and
                  Rachel Bittner and
                  Douglas Repetto and
                  Petr Viktorin and
                  Jo\~{ao} Felipe Santos and
                  Adrian Holovaty},
  title        = {librosa: 0.4.1. Zenodo. 10.5281/zenodo.18369},
  month        = oct,
  year         = 2015,
  doi          = {10.5281/zenodo.32193},
  url          = {http://dx.doi.org/10.5281/zenodo.32193}
}

@inproceedings{Mcfee2015-muda,
author = {McFee, Brian and Humphrey, Eric J. and Bello, Juan P.},
booktitle = {Proc. ISMIR},
title = {A software framework for musical data augmentation},
url = {https://bmcfee.github.io/papers/ismir2015{\_}augmentation.pdf},
year = {2015}
}

@article{Newton2012,
abstract = {Physiological evidence suggests that sound onset detection in the auditory system may be performed by specialized neurons as early as the cochlear nucleus. Psychoacoustic evidence shows that the sound onset can be important for the recognition of musical sounds. Here the sound onset is used in isolation to form tone descriptors for a musical instrument classification task. The task involves 2085 isolated musical tones from the McGill dataset across five instrument categories. A neurally inspired tone descriptor is created using a model of the auditory system's response to sound onset. A gammatone filterbank and spiking onset detectors, built from dynamic synapses and leaky integrate-and-fire neurons, create parallel spike trains that emphasize the sound onset. These are coded as a descriptor called the onset fingerprint. Classification uses a time-domain neural network, the echo state network. Reference strategies, based upon mel-frequency cepstral coefficients, evaluated either over the whole tone or only during the sound onset, provide context to the method. Classification success rates for the neurally-inspired method are around 75{\%}. The cepstral methods perform between 73{\%} and 76{\%}. Further testing with tones from the Iowa MIS collection shows that the neurally inspired method is considerably more robust when tested with data from an unrelated dataset.},
author = {Newton, Michael J. and Smith, Leslie S.},
doi = {10.1121/1.4707535},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/e67f5e0b6f7a8c5f649e2d22b479c2a5c12d4fbe.pdf:pdf},
issn = {00014966},
journal = {JASA},
number = {6},
pages = {4785},
pmid = {22712950},
title = {A neurally inspired musical instrument classification system based upon the sound onset},
url = {http://dspace.stir.ac.uk/bitstream/1893/21796/1/FINAL{\_}JAS004785.pdf},
volume = {131},
year = {2012}
}

@article{Patil2012,
  title={Music in our ears: the biological bases of musical timbre perception},
  author={Patil, Kailash and Pressnitzer, Daniel and Shamma, Shihab and Elhilali, Mounya},
  journal={PLoS Comput. Biol.},
  volume={8},
  number={11},
  pages={e1002759},
  year={2012},
  publisher={Public Library of Science}
}

@techreport{Peeters2004,
title = {{A large set of audio features for sound description (similarity and classification)
in the CUIDADO project}},
author = {Peeters, Geoffroy},
institution = {Ircam},
year = {2004}
}

@inproceedings{Schluter2014,
  title={Improved musical onset detection with convolutional neural networks},
  author={Schl\"{u}ter, Jan and B\"{o}ck, Sebastian},
  booktitle={Proc. ICASSP},
  year={2014},
}

@article{Sigtia2015,
abstract = {{We present a neural network model for polyphonic music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language mode{\}}. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony or the number or type of instruments. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We investigate various neural network architectures for the acoustic models and compare their performance to two popular state-of-the-art acoustic models. We also present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications. We evaluate the model's performance on the MAPS dataset and show that the proposed model outperforms state-of-the-art transcription systems.},
archivePrefix = {arXiv},
arxivId = {1508.01774},
author = {Sigtia, Siddharth and Benetos, Emmanouil and Dixon, Simon},
eprint = {1508.01774},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/93fbcd07f037e483b17cd65361d05680d8890bf8.pdf:pdf},
title = {An End-to-End Neural Network for Polyphonic Music Transcription},
journal = {arXiv preprint},
url = {http://arxiv.org/abs/1508.01774},
volume = {1508.01774},
year = {2015}
}

@article{Stowell2014,
  title={Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning},
  author={Stowell, Dan and Plumbley, Mark D.},
  journal={PeerJ},
  volume={2},
  pages={e488},
  year={2014},
  publisher={PeerJ Inc.}
}

@inproceedings{Ullrich2014,
author = {Ullrich, Karen and Schl{\"{u}}ter, Jan and Grill, Thomas},
booktitle = {Proc. ISMIR},
title = {Boundary detection in music structure analysis using convolutional neural networks},
url = {http://www.ofai.at/{~}jan.schlueter/pubs/2014{\_}ismir.pdf},
year = {2014}
}


@inproceedings{vandenOord2013,
title = {Deep content-based music recommendation},
author = {van den Oord, Aaron and Dieleman, Sander and Schrauwen, Benjamin},
booktitle = {Proc. NIPS},
year = {2013},
url = {http://papers.nips.cc/paper/5004-deep-content-based-music-recommendation.pdf}
}
